[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Nextflow and running nf-core workflows",
    "section": "",
    "text": "This workshop is designed to provide participants with a foundational understanding of Nextflow and nf-core pipelines, with a focus on running existing pipelines efficiently. Participants are expected to have prior experience with the command-line interface and working with cluster systems like Slurm. The primary goal of the workshop is to equip researchers with the skills needed to use nf-core pipelines for their research data.\nIt is not intended for those looking to develop new Nextflow workflows\n\nCourse Presenters\n\nRichard Lupat, Bioinformatics Core Facility\nMiriam Yeung, Cancer Genomics Translational Research Centre\n\n\n\nCourse Helpers\n\nSanduni Rajapaksa, Research Computing Facility\nHenrietta Holze, Dawson Lab\n\n\n\nPrerequisites\n\nExperience with command line interface and cluster/slurm\nFamiliarity with the basic concept of workflows\nAccess to Peter Mac Cluster\n\n\n\nLearning Objectives:\nBy the end of this workshop, participants should be able to:\n\nGain exposure to key concepts and terminology in Nextflow and nf-core pipelines.\nUnderstand the foundational knowledge required to navigate and customize the code base of nf-core pipelines.\nDevelop basic troubleshooting and customization skills necessary for responsibly applying nf-core pipelines to your own research data.\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP via Slack/Teams.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\nTime\n\n\n\n\nSetup\nFollow these instructions to install VS Code and setup your workspace\nPrior to workshop\n\n\nSession kick off\nSession kick off: Discuss learning outcomes and finalising workspace setup\n09:30 - 09:40\n\n\nIntroduction to Nextflow\nIntroduction to Nextflow: Introduce nextflow‚Äôs core features and concepts; including CLI and how to run it on Peter Mac cluster\n09:40 - 10:20\n\n\nIntroduction to nf-core\nIntroduction to nf-core: Introduce nf-core features and concepts, structures, tools, and example nf-core pipelines\n10:20 - 11:00\n\n\nBreak\nBreak\n11:00 - 11:15\n\n\nCustomising & running nf-core pipelines\nCustomising & running nf-core pipelines: Discuss pipelines‚Äô required inputs, optional inputs, outputs, parameters file and configurations files\n11:15 - 11:45\n\n\nTroubleshooting nextflow run\nTroubleshooting nextflow run: Discuss Nextflow logging, caching, task execution directory, dependencies, and manual troubleshooting\n11:45 - 12:15\n\n\nBest practise and Q&A\nBest practise, tips & tricks for running nextflow pipelines at Peter Mac cluster\n12:15 - 12:45\n\n\n\n\n\nCredits and acknowledgement\nThis workshop is adapted from Customising Nf-Core Workshop materials from Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/2.3_tips_and_tricks.html",
    "href": "workshops/2.3_tips_and_tricks.html",
    "title": "Best practise, tips and tricks",
    "section": "",
    "text": "2.3.1. Running Nextflow Pipelines on a HPC \nNextflow, by default, spawns parallel task executions wherever it is running. You can use Nextflow‚Äôs executors feature to run these tasks using an HPC job schedulers such as SLURM and PBS Pro. Use a custom configuration file to send all processes to the job scheduler as separate jobs and define essential resource requests like cpus, time, memory, and queue inside a process {} scope.\n\nRun all workflow tasks as separate jobs on HPC\nIn this custom configuration file we have sent all tasks that a workflow is running to a PBS Pro job scheduler and specified jobs to be run on the normal queue, each running for a max time of 3 hours with 1 cpu and 4 Gb of memory:\nprocess {\n  executor = 'slurm'\n  queue = 'prod_short'\n  cpus = 1\n  time = '2h'\n  memory = '4.GB'\n}\n\n\nRun processes with different resource profiles as HPC jobs\nAdjusting the custom configuration file above, we can use the withName {} process selector to specify process-specific resource requirements:\nprocess {\n  executor = 'slurm'\n    \n  withName processONE {\n    queue = 'prod_short'\n    cpus = 1\n    time = '2h'\n    memory = '4.GB'\n  }\n\n  withName processTWO {\n    queue = 'prod_med'\n    cpus = 2\n    time = '10h'\n    memory = '50.GB'\n  }\n}\n\n\nSpecify infrastructure-specific directives for your jobs\nAdjusting the custom configuration file above, we can define any native configuration options using the clusterOptions directive. We can use this to specify non-standard resources. Below we have specified which HPC project code to bill for all process jobs:\nYou can also setup a config to tailor specific to Peter Mac‚Äôs HPC partitions setup.\nexecutor {\n    queueSize         = 100\n    queueStatInterval = '1 min'\n    pollInterval      = '1 min'\n    submitRateLimit   = '20 min'\n}\n\nprocess {\n    executor = 'slurm'\n    cache    = 'lenient'\n    beforeScript = 'module load singularity'\n    stageInMode = 'symlink'\n    queue = { task.time &lt; 2.h ? 'prod_short' : task.time &lt; 24.h ? 'prod_med' : 'prod' } \n}\n\n\n\n\n\n\nChallenge\n\n\n\nRun the previous nf-core/rnaseq workflow using the process and executor scope above (in a config file), and send each task to slurm.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a nextflow.config file\nprocess.executor = 'slurm'\nRun the nfcore/rna-seq workflow again\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    -params-file workshop-params.yaml\n    -profile singularity \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \nDid you get the following error?\nsbatch: error: Batch job submission failed: Access/permission denied\nTry running the same workflow on login-node and observe the difference\n&gt;&gt;&gt; squeue -u rlupat -i 5\n\n          17429286      prod nf-NFCOR   rlupat  R       0:03      1 papr-res-compute01\n\n\n\n\n\n\n\n2.3.2. Things to note for Peter Mac Cluster\n\nBest not to launch nextflow on a login-node\nEven though nextflow is not supposed to be doing any heavy computation, nextflow still consume CPUs and memory to do some of the operations. Our login node is not designed to handle multiple users running a Groovy applicaation that spawn further operations.\nIn saying that, launching nextflow from a compute node is also not possible from our previous exercise. So what is the solution?\nOur cluster prohibits compute nodes from spawning new jobs. There are only two partitions that are currently available to spawn new jobs janis and janis-dev. Therefore, if you are submitting your nextflow pipeline in an sbatch file, it is probably good to point that to the janis node.\n\n\nSet your working directory to a scratch space\nWhen we launch a nextflow workflow, by default it will use the current directory to create a work directory and all the intermediate files will be stored there, only to be cleaned at completion. This means that if you run a long running workflow, chances are your intermediate files will be sent to the tape long term archiving. There are also benefits for running in scratch, as we are using a faster spinning disk, resulting in a faster execution.\n\n\n\n2.3.3. Clean your work directory\nYour work directory can get very big very quickly (especially if you are using full sized datasets). It is good practise to clean your work directory regularly. Rather than removing the work folder with all of it‚Äôs contents, the Nextflow clean function allows you to selectively remove data associated with specific runs.\nnextflow clean -help\nThe -after, -before, and -but options are all very useful to select specific runs to clean. The -dry-run option is also very useful to see which files will be removed if you were to -force the clean command.\n\n\n\n\n\n\nChallenge\n\n\n\nYou Nextflow to clean your work work directory of staged files but keep your execution logs.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextflow clean command with the -k and -f options:\nnextflow clean -k -f\n\n\n\n\n\n\n2.3.4. Change default Nextflow cache strategy\nWorkflow execution is sometimes not resumed as expected. The default behaviour of Nextflow cache keys is to index the input files meta-data information. Reducing the cache stringency to lenient means the files cache keys are based only on filesize and path, and can help to avoid unexpectedly re-running certain processes when -resume is in use.\nTo apply lenient cache strategy to all of your runs, you could add to a custom configuration file:\nprocess {\n    cache = 'lenient'\n}\nYou can specify different cache stategies for different processes by using withName or withLabel. You can specify a particular cache strategy be applied to certain profiles within your institutional config, or to apply to all profiles described within that config by placing the above process code block outside the profiles scope.\n\n\n2.3.5. Access private GitHub repositories\nTo interact with private repositories on GitHub, you can provide Nextflow with access to GitHub by specifying your GitHub user name and a Personal Access Token in the scm configuration file inside your specified .nextflow/ directory:\nproviders {\n\n  github {\n    user = 'rlupat'\n    password = 'my-personal-access-token'\n  }\n\n}\n\n\n2.3.6. Nextflow Tower\nBioCommons Tower Instance\n\n\n2.3.7. Additional resources \nHere are some useful resources to help you get started with running nf-core pipelines and developing Nextflow pipelines:\n\nNextflow tutorials\nnf-core pipeline tutorials\nNextflow patterns\nHPC tips and tricks\nNextflow coding best practice recommendations\nThe Nextflow blog\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/1.2_intro_nf_core.html",
    "href": "workshops/1.2_intro_nf_core.html",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the core features of nf-core.\nLearn the terminology used by nf-core.\nUse Nextflow to pull and run the nf-core/testpipeline workflow\n\n\n\nIntroduction to nf-core: Introduce nf-core features and concepts, structures, tools, and example nf-core pipelines\n\n1.2.1. What is nf-core?\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.\nThe community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276‚Äì278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won‚Äôt be left in the dark.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.\n\nCloud-ready\n\nnf-core workflows are tested on AWS\n\n\n\n\n1.2.2. Executing an nf-core workflow\nThe nf-core website has a full list of workflows and asssociated documentation tno be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction\n\nAn introduction and overview of the workflow\n\nResults\n\nExample output files generated from the full test dataset\n\nUsage docs\n\nDescriptions of how to execute the workflow\n\nParameters\n\nGrouped workflow parameters with descriptions\n\nOutput docs\n\nDescriptions and examples of the expected output files\n\nReleases & Statistics\n\nWorkflow version history and statistics\n\n\nAs nf-core is a community development project the code for a pipeline can be changed at any time. To ensure that you have locked in a specific version of a pipeline you can use Nextflow‚Äôs built-in functionality to pull a workflow. The Nextflow pull command can download and cache workflows from GitHub repositories:\nnextflow pull nf-core/&lt;pipeline&gt;\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/&lt;pipeline&gt;\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the -revision or -r flag.\nFor this section of the workshop we will be using the nf-core/testpipeline as an example.\nAs we will be running some bioinformatics tools, we will need to make sure of the following:\n\nWe are not running on login node\nsingularity module is loaded (module load singularity/3.7.3)\n\n\n\n\n\n\n\nSetup an interactive session\n\n\n\nsrun --pty -p prod_short --mem 20GB --cpus-per-task 2 -t 0-2:00 /bin/bash\n\nEnsure the required modules are loaded\nmodule list\nCurrently Loaded Modulefiles:\n  1) java/jdk-17.0.6      2) nextflow/23.04.1     3) squashfs-tools/4.5   4) singularity/3.7.3\n\n\n\nWe will also create a separate output directory for this section.\ncd /scratch/users/&lt;your-username&gt;/nfWorkshop; mkdir ./lesson1.2 && cd $_\nThe base command we will be using for this section is:\nnextflow run nf-core/testpipeline -profile test,singularity --outdir my_results\n\n\n1.2.3. Workflow structure\nnf-core workflows start from a common template and follow the same structure. Although you won‚Äôt need to edit code in the workflow project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution.\nLet‚Äôs take a look at the code for the nf-core/rnaseq pipeline.\nNextflow DSL2 workflows are built up of subworkflows and modules that are stored as separate .nf files.\nMost nf-core workflows consist of a single workflow file (there are a few exceptions). This is the main &lt;workflow&gt;.nf file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules.\nA subworkflow is a groups of modules that are used in combination with each other and have a common purpose. Subworkflows improve workflow readability and help with the reuse of modules within a workflow. The nf-core community also shares subworkflows in the nf-core subworkflows GitHub repository. Local subworkflows are workflow specific that are not shared in the nf-core subworkflows repository.\nLet‚Äôs take a look at the BAM_STATS_SAMTOOLS subworkflow.\nThis subworkflow is comprised of the following modules: - SAMTOOLS_STATS - SAMTOOLS_IDXSTATS, and - SAMTOOLS_FLAGSTAT\nA module is a wrapper for a process, most modules will execute a single tool and contain the following definitions: - inputs - outputs, and - script block.\nLike subworkflows, modules can also be shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility. Local modules are workflow specific that are not shared in the nf-core modules repository.\n\n\n1.2.4. Viewing parameters\nEvery nf-core workflow has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will have additional text to help you understand when and how a parameter should be used.\n\n\n\n\n\nParameters and their descriptions can also be viewed in the command line using the run command with the --help parameter:\nnextflow run nf-core/&lt;workflow&gt; --help\n\n\n\n\n\n\nChallenge\n\n\n\nView the parameters for the nf-core/testpipeline workflow using the command line:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe nf-core/testpipeline workflow parameters can be printed using the run command and the --help option:\nnextflow run nf-core/testpipeline --help\n\n\n\n\n\n1.2.5. Parameters in the command line\nParameters can be customized using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (--):\nnextflow run nf-core/&lt;workflow&gt; --&lt;parameter&gt;\n\n\n\n\n\n\nTip\n\n\n\nNextflow options are prefixed with a single dash (-) and workflow parameters are prefixed with a double dash (--).\n\n\nDepending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag:\nnextflow run nf-core/&lt;workflow&gt; --&lt;parameter&gt; string\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the nf-core/testpipeline workflow the name of your favorite animal using the multiqc_title parameter using a command line flag:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd the --multiqc_title flag to your command and execute it. Use the -resume option to save time:\nnextflow run nf-core/testpipeline -profile test,singularity --multiqc_title koala --outdir my_results -resume\n\n\n\nIn this example, you can check your parameter has been applied by listing the files created in the results folder (my_results):\nls my_results/multiqc/\n\n\n1.2.6. Configuration files\nConfiguration files are .config files that can contain various workflow properties. Custom paths passed in the command-line using the -c option:\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -c &lt;path/to/custom.config&gt;\nMultiple custom .config files can be included at execution by separating them with a comma (,).\nCustom configuration files follow the same structure as the configuration file included in the workflow directory. Configuration properties are organized into scopes by grouping the properties in the same scope using the curly brackets notation. For example:\nalpha {\n     x = 1\n     y = 'string value..'\n}\nScopes allow you to quickly configure settings required to deploy a workflow on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a workflow on a HPC cluster. Similarly, the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the nf-core/testpipeline workflow the name of your favorite color using the multiqc_title parameter in a custom my_custom.config file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom my_custom.config file that contains your favourite colour, e.g., blue:\nparams {\n    multiqc_title = \"blue\"\n}\nInclude the custom .config file in your execution command with the -c option:\nnextflow run nf-core/testpipeline --outdir my_results -profile test,singularity -resume -c my_custom.config\nCheck that it has been applied:\nls my_results/multiqc/\nWhy did this fail?\nYou can not use the params scope in custom configuration files. Parameters can only be configured using the -params-file option and the command line. While parameter is listed as a parameter on the STDOUT, it was not applied to the executed command.\nWe will revisit this at the end of the module\n\n\n\n\n\n1.2.7 Parameter files\nParameter files are used to define params options for a pipeline, generally written in the YAML format. They are added to a pipeline with the flag --params-file\nExample YAML:\n\"&lt;parameter1_name&gt;\": 1,\n\"&lt;parameter2_name&gt;\": \"&lt;string&gt;\",\n\"&lt;parameter3_name&gt;\": true\n\n\n\n\n\n\nChallenge\n\n\n\nBased on the failed application of the parameter multiqc_title create a my_params.yml setting multiqc_title to your favourite colour. Then re-run the pipeline with the your my_params.yml\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSet up my_params.yml\nmultiqc_title: \"black\"\nnextflow run nf-core/testpipeline -profile test,singularity --params-file my_params.yml --outdir Lesson1_2\n\n\n\n\n\n1.2.8. Default configuration files\nAll parameters will have a default setting that is defined using the nextflow.config file in the workflow project directory. By default, most parameters are set to null or false and are only activated by a profile or configuration file.\nThere are also several includeConfig statements in the nextflow.config file that are used to load additional .config files from the conf/ folder. Each additional .config file contains categorized configuration information for your workflow execution, some of which can be optionally included:\n\nbase.config\n\nIncluded by the workflow by default.\nGenerous resource allocations using labels.\nDoes not specify any method for software management and expects software to be available (or specified elsewhere).\n\nigenomes.config\n\nIncluded by the workflow by default.\nDefault configuration to access reference files stored on AWS iGenomes.\n\nmodules.config\n\nIncluded by the workflow by default.\nModule-specific configuration options (both mandatory and optional).\n\n\nNotably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a workflow by using the -profile command option:\nnextflow run nf-core/&lt;workflow&gt; -profile &lt;profile&gt;\nProfiles used by nf-core workflows include:\n\nSoftware management profiles\n\nProfiles for the management of software using software management tools, e.g., docker, singularity, and conda.\n\nTest profiles\n\nProfiles to execute the workflow with a standardized set of test data and parameters, e.g., test and test_full.\n\n\nMultiple profiles can be specified in a comma-separated (,) list when you execute your command. The order of profiles is important as they will be read from left to right:\nnextflow run nf-core/&lt;workflow&gt; -profile test,singularity\nnf-core workflows are required to define software containers and conda environments that can be activated using profiles.\n\n\n\n\n\n\nTip\n\n\n\nIf you‚Äôre computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core workflow with the test profile and the respective software management profile ‚Äòout of the box‚Äô. The test data profile will pull small test files directly from the nf-core/test-data GitHub repository and run it on your local system. The test profile is an important control to check the workflow is working as expected and is a great way to trial a workflow. Some workflows have multiple test profiles for you to test.\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nNextflow can be used to pull nf-core workflows.\nnf-core workflows follow similar structures\nnf-core workflows are configured using parameters and profiles\n\n\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/1.1_intro_nextflow.html",
    "href": "workshops/1.1_intro_nextflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the benefits of a workflow manager.\nLearn Nextflow terminology.\nLearn basic commands and options to run a Nextflow workflow"
  },
  {
    "objectID": "workshops/1.1_intro_nextflow.html#footnotes",
    "href": "workshops/1.1_intro_nextflow.html#footnotes",
    "title": "Introduction to Nextflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.lexico.com/definition/workflow‚Ü©Ô∏é"
  },
  {
    "objectID": "workshops/2.2_troubleshooting.html",
    "href": "workshops/2.2_troubleshooting.html",
    "title": "Troubleshooting Nextflow run",
    "section": "",
    "text": "2.2.1. Nextflow log\nIt is important to keep a record of the commands you have run to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow log command:\nnextflow log\nThe log command has multiple options to facilitate the queries and is especially useful while debugging a workflow and inspecting execution metadata. You can view all of the possible log options with -h flag:\nnextflow log -h\nTo query a specific execution you can use the RUN NAME or a SESSION ID:\nnextflow log &lt;run name&gt;\nTo get more information, you can use the -f option with named fields. For example:\nnextflow log &lt;run name&gt; -f process,hash,duration\nThere are many other fields you can query. You can view a full list of fields with the -l option:\nnextflow log -l\n\n\n\n\n\n\nChallenge\n\n\n\nUse the log command to view with process, hash, and script fields for your tasks from your most recent Nextflow execution.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the log command to get a list of you recent executions:\nnextflow log\nTIMESTAMP           DURATION    RUN NAME            STATUS  REVISION ID SESSION ID                              COMMAND \n2023-11-21 22:43:14 14m 17s     jovial_angela       OK      3bec2331ca  319751c3-25a6-4085-845c-6da28cd771df    nextflow run nf-core/rnaseq\n2023-11-21 23:05:49 1m 36s      marvelous_shannon   OK      3bec2331ca  319751c3-25a6-4085-845c-6da28cd771df    nextflow run nf-core/rnaseq\n2023-11-21 23:10:00 1m 35s      deadly_babbage      OK      3bec2331ca  319751c3-25a6-4085-845c-6da28cd771df    nextflow run nf-core/rnaseq\nQuery the process, hash, and script using the -f option for the most recent run:\nnextflow log marvelous_shannon -f process,hash,script\n\n[... truncated ...]\n\nNFCORE_RNASEQ:RNASEQ:SUBREAD_FEATURECOUNTS  7c/f936d4   \n    featureCounts \\\n        -B -C -g gene_biotype -t exon \\\n        -p \\\n        -T 2 \\\n        -a chr22_with_ERCC92.gtf \\\n        -s 2 \\\n        -o HBR_Rep1_ERCC.featureCounts.txt \\\n        HBR_Rep1_ERCC.markdup.sorted.bam\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"NFCORE_RNASEQ:RNASEQ:SUBREAD_FEATURECOUNTS\":\n        subread: $( echo $(featureCounts -v 2&gt;&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n\n[... truncated ... ]\n\nNFCORE_RNASEQ:RNASEQ:MULTIQC    7a/8449d7   \n    multiqc \\\n        -f \\\n         \\\n         \\\n        .\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"NFCORE_RNASEQ:RNASEQ:MULTIQC\":\n        multiqc: $( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \n\n\n\n\n\n2.2.2. Execution cache and resume\nTask execution caching is an essential feature of modern workflow managers. As such, Nextflow provides an automated caching mechanism for every execution. When using the Nextflow -resume option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks.\nNextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID‚Äôs are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you.\nYou can re-launch the previously executed nf-core/rnaseq workflow again, but with a -resume flag, and observe the progress. Notice the time it takes to complete the workflow.\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    --input samplesheet.csv \\\n    --outdir ./my_results \\\n    --fasta $materials/ref/chr22_with_ERCC92.fa \\\n    --gtf $materials/ref/chr22_with_ERCC92.gtf \\\n    -profile singularity \\\n    --skip_markduplicates true \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \n\n[80/ec6ff8] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF2BED (chr22_with_ERCC92.gtf)                  [100%] 1 of 1, cached: 1 ‚úî\n[1a/7bec9c] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_GENE_FILTER (chr22_with_ERCC92.fa)           [100%] 1 of 1, cached: 1 ‚úî\nExecuting this workflow will create a my_results directory with selected results files and add some further sub-directories into the work directory\nIn the schematic above, the hexadecimal numbers, such as 80/ec6ff8, identify the unique task execution. These numbers are also the prefix of the work directories where each task is executed.\nYou can inspect the files produced by a task by looking inside the work directory and using these numbers to find the task-specific execution path:\nls work/80/ec6ff8ba69a8b5b8eede3679e9f978/\nIf you look inside the work directory of a FASTQC task, you will find the files that were staged and created when this task was executed:\n&gt;&gt;&gt; ls -la  work/e9/60b2e80b2835a3e1ad595d55ac5bf5/ \n\ntotal 15895\ndrwxrwxr-x 2 rlupat rlupat    4096 Nov 22 03:39 .\ndrwxrwxr-x 4 rlupat rlupat    4096 Nov 22 03:38 ..\n-rw-rw-r-- 1 rlupat rlupat       0 Nov 22 03:39 .command.begin\n-rw-rw-r-- 1 rlupat rlupat    9509 Nov 22 03:39 .command.err\n-rw-rw-r-- 1 rlupat rlupat    9609 Nov 22 03:39 .command.log\n-rw-rw-r-- 1 rlupat rlupat     100 Nov 22 03:39 .command.out\n-rw-rw-r-- 1 rlupat rlupat   10914 Nov 22 03:39 .command.run\n-rw-rw-r-- 1 rlupat rlupat     671 Nov 22 03:39 .command.sh\n-rw-rw-r-- 1 rlupat rlupat     231 Nov 22 03:39 .command.trace\n-rw-rw-r-- 1 rlupat rlupat       1 Nov 22 03:39 .exitcode\nlrwxrwxrwx 1 rlupat rlupat      63 Nov 22 03:39 HBR_Rep1_ERCC_1.fastq.gz -&gt; HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz\n-rw-rw-r-- 1 rlupat rlupat    2368 Nov 22 03:39 HBR_Rep1_ERCC_1.fastq.gz_trimming_report.txt\n-rw-rw-r-- 1 rlupat rlupat  697080 Nov 22 03:39 HBR_Rep1_ERCC_1_val_1_fastqc.html\n-rw-rw-r-- 1 rlupat rlupat  490526 Nov 22 03:39 HBR_Rep1_ERCC_1_val_1_fastqc.zip\n-rw-rw-r-- 1 rlupat rlupat 6735205 Nov 22 03:39 HBR_Rep1_ERCC_1_val_1.fq.gz\nlrwxrwxrwx 1 rlupat rlupat      63 Nov 22 03:39 HBR_Rep1_ERCC_2.fastq.gz -&gt; HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz\n-rw-rw-r-- 1 rlupat rlupat    2688 Nov 22 03:39 HBR_Rep1_ERCC_2.fastq.gz_trimming_report.txt\n-rw-rw-r-- 1 rlupat rlupat  695591 Nov 22 03:39 HBR_Rep1_ERCC_2_val_2_fastqc.html\n-rw-rw-r-- 1 rlupat rlupat  485732 Nov 22 03:39 HBR_Rep1_ERCC_2_val_2_fastqc.zip\n-rw-rw-r-- 1 rlupat rlupat 7088948 Nov 22 03:39 HBR_Rep1_ERCC_2_val_2.fq.gz\nlrwxrwxrwx 1 rlupat rlupat     102 Nov 22 03:39 HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz -&gt; /data/seqliner/test-data/rna-seq/fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz\nlrwxrwxrwx 1 rlupat rlupat     102 Nov 22 03:39 HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz -&gt; /data/seqliner/test-data/rna-seq/fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz\n-rw-rw-r-- 1 rlupat rlupat     109 Nov 22 03:39 versions.yml\nThe FASTQC process runs twice, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [e9/60b2e8] represents just one of the four sets of input data that was processed.\nIt‚Äôs very likely you will execute a workflow multiple times as you find the parameters that best suit your data. You can save a lot of spaces (and time) by resuming a workflow from the last step that was completed successfully and/or unmodified.\nIn practical terms, the workflow is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are used as the process results.\nNotably, the -resume functionality is very sensitive. Even touching a file in the work directory can invalidate the cache.\n\n\n\n\n\n\nChallenge\n\n\n\nInvalidate the cache by touching a .fastq.gz file in a FASTQC task work directory (you can use the touch command). Execute the workflow again with the -resume option to show that the cache has been invalidated.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the workflow for the first time (if you have not already).\nUse the task ID shown for the FASTQC process and use it to find and touch a the sample1_R1.fastq.gz file:\ntouch work/ff/21abfa87cc7cdec037ce4f36807d32/HBR_Rep1_ERCC_1.fastq.gz\nExecute the workflow again with the -resume command option:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    --input samplesheet.csv \\\n    --outdir ./my_results \\\n    --fasta $materials/ref/chr22_with_ERCC92.fa \\\n    --gtf $materials/ref/chr22_with_ERCC92.gtf \\\n    -profile singularity \\\n    --skip_markduplicates true \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \nYou should see that some task were invalid and were executed again.\nWhy did this happen?\nIn this example, the cache of two FASTQC tasks were invalid. The fastq file we touch is used by in the pipeline in multiple places. Thus, touching the symlink for this file and changing the date of last modification disrupted two task executions.\n\n\n\n\n\n2.2.3. Troubleshoot warning and error messages\nWhile our previous workflow execution completed successfully, there were a couple of warning messages that may be cause for concern:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 20-Nov-2023 00:29:04\nDuration    : 10m 15s\nCPU hours   : 0.3 \nSucceeded   : 72\n\n\n\n\n\n\nHandling dodgy error messages ü§¨\n\n\n\nThe first warning message isn‚Äôt very descriptive (see this pull request). You might come across issues like this when running nf-core pipelines, too. Bug reports and user feedback is very important to open source software communities like nf-core. If you come across any issues, submit a GitHub issue or start a discussion in the relevant nf-core Slack channel so others are aware and it can be addressed by the pipeline‚Äôs developers.\n\n\n‚û§ Take a look at the MultiQC report, as directed by the second message. You can find the MultiQC report in the lesson2.1/ directory:\nls -la lesson2.1/multiqc/star_salmon/\ntotal 1402\ndrwxrwxr-x 4 rlupat rlupat    4096 Nov 22 00:29 .\ndrwxrwxr-x 3 rlupat rlupat    4096 Nov 22 00:29 ..\ndrwxrwxr-x 2 rlupat rlupat    8192 Nov 22 00:29 multiqc_data\ndrwxrwxr-x 5 rlupat rlupat    4096 Nov 22 00:29 multiqc_plots\n-rw-rw-r-- 1 rlupat rlupat 1419998 Nov 22 00:29 multiqc_report.html\n‚û§ Download the multiqc_report.html the file navigator panel on the left side of your VS Code window by right-clicking on it and then selecting Download. Open the file on your computer.\nTake a look a the section labelled WARNING: Fail Strand Check\nThe warning we have received is indicating that the read strandedness we specified in our samplesheet.csv and inferred strandedness identified by the RSeqQC process in the pipeline do not match. It looks like the test samplesheet have incorrectly specified strandedness as forward in the samplesheet.csv when our raw reads actually show an equal distribution of sense and antisense reads.\nFor those who are not familiar with RNAseq data, incorrectly specified strandedness may negatively impact the read quantification step (process: Salmon quant) and give us inaccurate results. So, let‚Äôs clarify how the Salmon quant process is gathering strandedness information for our input files by default and find a way to address this with the parameters provided by the nf-core/rnaseq pipeline.\n\n\n\n2.2.4. Identify the run command for a process\nTo observe exactly what command is being run for a process, we can attempt to infer this information from the module‚Äôs main.nf script in the modules/ directory. However, given all the different parameters that may be applied at the process level, this may not be very clear.\n‚û§ Take a look at the Salmon quant main.nf file:\nnf-core-rnaseq-3.11.1/workflow/modules/nf-core/salmon/quant/main.nf\nUnless you are familiar with developing nf-core pipelines, it can be very hard to see what is actually happening in the code, given all the different variables and conditional arguments inside this script. Above the script block we can see strandedness is being applied using a few different conditional arguments. Instead of trying to infer how the $strandedness variable is being defined and applied to the process, let‚Äôs use the hidden command files saved for this task in the work/ directory.\n\n\n\n\n\n\nHidden files in the work directory!\n\n\n\nRemember that the pipeline‚Äôs results are cached in the work directory. In addition to the cached files, each task execution directories inside the work directory contains a number of hidden files:\n\n.command.sh: The command script run for the task.\n.command.run: The command wrapped used to run the task.\n.command.out: The task‚Äôs standard output log.\n.command.err: The task‚Äôs standard error log.\n.command.log: The wrapper execution output.\n.command.begin: A file created as soon as the job is launched.\n.exitcode: A file containing the task exit code (0 if successful)\n\n\n\nWith nextflow log command that we discussed previously, there are multiple options to facilitate the queries and is especially useful while debugging a pipeline and while inspecting pipeline execution metadata.\nTo understand how Salmon quant is interpreting strandedness, we‚Äôre going to use this command to track down the hidden .command.sh scripts for each Salmon quant task that was run. This will allow us to find out how Salmon quant handles strandedness and if there is a way for us to override this.\n‚û§ Use the Nextflow log command to get the unique run name information of the previously executed pipelines:\nnextflow log &lt;run-name&gt;\nThat command will list out all the work subdirectories for all processes run.\nAnd we now need to find the specific hidden.command.sh for Salmon tasks. But how to find them? ü§î\n‚û§ Let‚Äôs add some custom bash code to query a Nextflow run with the run name from the previous lesson. First, save your run name in a bash variable. For example:\nrun_name=marvelous_shannon\n‚û§ And let‚Äôs save the tool of interest (salmon) in another bash variable to pull it from a run command:\ntool=salmon\n‚û§ Next, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2&gt;/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‚Äòsalmon‚Äô. There are a few different processes that run Salmon to perform other steps in the workflow. We are looking for Salmon quant which performs the read quantification:\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/57/fba8f9a2385dac5fa31688ba1afa9b/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/30/0113a58c14ca8d3099df04ebf388f3/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/ec/95d6bd12d578c3bce22b5de4ed43fe/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/49/6fedcb09e666432ae6ddf8b1e8f488/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/b4/2ca8d05b049438262745cde92955e9/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/38/875d68dae270504138bb3d72d511a7/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/72/776810a99695b1c114cbb103f4a0e6/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/1c/dc3f54cc7952bf55e6742dd4783392/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/f3/5116a5b412bde7106645671e4c6ffb/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/17/fb0c791810f42a438e812d5c894ebf/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/4c/931a9b60b2f3cf770028854b1c673b/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/91/e1c99d8acb5adf295b37fd3bbc86a5/.command.sh\nCompared with the salmon quant main.nf file, we get a lot more fine scale details from the .command.sh process scripts:\n&gt;&gt;&gt; cat main.nf\nsalmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n&gt;&gt;&gt; cat .command.sh\nsalmon quant \\\n    --geneMap chr22_with_ERCC92.gtf \\\n    --threads 2 \\\n    --libType=ISF \\\n    -t genome.transcripts.fa \\\n    -a HBR_Rep1_ERCC.Aligned.toTranscriptome.out.bam \\\n     \\\n    -o HBR_Rep1_ERCC\nLooking at the nf-core/rnaseq Parameter documentation and Salmon documentation, we found that we can override this default using the --salmon_quant_libtype A parameter to indicate our data is unstranded and override samplesheet.csv input.\n\n\n\n\n\n\nHow do I get rid of the strandedness check warning message?\n\n\n\nIf we want to get rid of the warning message Please check MultiQC report: 2/2 samples failed strandedness check, we‚Äôll have to change the strandedness fields in our samplesheet.csv. Keep in mind, doing this will invalidate the pipeline‚Äôs cache and cause the pipeline to run from the beginning.\n\n\n\n\n\n2.2.5. Write a parameter file\nFrom the previous section we learn that Nextflow accepts either yaml or json formats for parameter files. Any of the pipeline-specific parameters can be supplied to a Nextflow pipeline in this way.\n\n\n\n\n\n\nChallenge\n\n\n\nFill in the parameters file below and save as workshop-params.yaml. This time, include the --salmon_quant_libtype A parameter.\nüí° YAML formatting tips!\n\nStrings need to be inside double quotes\nBooleans (true/false) and numbers do not require quotes\n\ninput: \"\"\noutdir: \"lesson2.2\"\nfasta: \"\"\ngtf: \"\"\nstar_index: \"\"\nsalmon_index: \"\"\nskip_markduplicates: \nsave_trimmed: \nsave_unaligned: \nsalmon_quant_libtype: \"A\" \n\n\n\n\n2.2.6. Apply the parameter file\n‚û§ Once your params file has been saved, run:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    -params-file workshop-params.yaml\n    -profile singularity \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \nThe number of pipeline-specific parameters we‚Äôve added to our run command has been significantly reduced. The only -- parameters we‚Äôve provided to the run command relate to how the pipeline is executed on our interative job. These resource limits won‚Äôt be applicable to others who will run the pipeline on a different infrastructure.\nAs the workflow runs a second time, you will notice 4 things:\n\nThe command is much tidier thanks to offloading some parameters to the params file\nThe -resume flag. Nextflow has lots of run options including the ability to use cached output!\nSome processes will be pulled from the cache. These processes remain unaffected by our addition of a new parameter.\n\nThis run of the pipeline will complete in a much shorter time.\n\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 05:58:06\nDuration    : 1m 51s\nCPU hours   : 0.3 (82.2% cached)\nSucceeded   : 11\nCached      : 55\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/00_setup.html",
    "href": "workshops/00_setup.html",
    "title": "Essential Workshop Preparation",
    "section": "",
    "text": "In this workshop, we will be using Peter Mac‚Äôs HPC to run nextflow and nf-core workflows.\nBefore joining the workshop, please complete the following checklist:"
  },
  {
    "objectID": "workshops/00_setup.html#option-1-install-and-set-up-visual-studio-code",
    "href": "workshops/00_setup.html#option-1-install-and-set-up-visual-studio-code",
    "title": "Essential Workshop Preparation",
    "section": "Option 1: Install and set up Visual Studio Code",
    "text": "Option 1: Install and set up Visual Studio Code\nWe recommend Visual Studio Code as a source code editor because it is lightweight and has rich support for extensions and syntax highlighting available across various popular operating system.\nDownload Visual Studio Code on your computer and follow the instructions for your specific Operating System as required:\n\nmacOS\nWindows\nLinux\n\nOnce installed, open VS Code on your computer.\n\n\nInstall the Nextflow Extension\nThe Nextflow extension provides syntax highlighting and quick shortcuts for common code snippets.\nClick on the extensions button (four blocks) on the left side bar. Search for ‚ÄúNextflow‚Äù in the extensions search bar, then click on the blue ‚ÄúInstall‚Äù button.\n\n\n\nInstall the Remote-SSH Extension\nRemote-SSH allows us to use any remote machine with a SSH server as your development environment. This lets us work directly on the our cluster‚Äôs storage.\nClick on the extensions button (four blocks) on the left side bar. Search for ‚ÄúRemote - SSH‚Äù in the extensions search bar, then click on the blue ‚ÄúInstall‚Äù button.\n\n\n\nLogin via Visual Studio Code\nConnect to your instance with VS code by adding the host details to your .ssh config file (if you have not done this previously)\n\nIn a new VS code window, type Ctrl+Shift+P if you‚Äôre on a Windows machine or Cmd+Shift+P for MacOS to open the command palette\nSelect Remote-SSH: Open SSH configuration file and select your .ssh config file\nAdd a new entry with your details to login to cluster, and save your .ssh config file:\n\nHost pmac-cluster\n    HostName kvmpr-res-lnode1.unix.petermac.org.au\n    User &lt;your-cluster-user-name&gt;\n\nType Ctrl+Shift+P and select Remote-SSH: Connect to Host; and pmac-cluster (or whatever you name your host above)\nWhen prompted, select Linux as the platform of the remote host from the dropdown menu\nType in your password and hit enter\n\nHaving successfully logged in, you should see a small blue or green box in the bottom left corner of your screen:\n\nTo set up your VS Code window for the workshop:\n\nOpen a new folder in the file explorer panel on the left side of the screen by typing Ctrl + K, Ctrl + O if you‚Äôre running Windows or Cmd+K+ Cmd + O for MacOS\nSelect /scratch/users/&lt;your-username&gt;/nfWorkshop to open our working directory. If you encountered an error that the directory does not exist, you would need to ssh in to the cluster and create that directory first before attempting this step.\nWhen prompted, select the box for Trust the authors of all files in the parent folder ‚Äòhome‚Äô then click Yes, I trust the authors\nYou can dismiss the warning message saying our cluster‚Äôs git version is outdated\nTo open a terminal, type Ctrl+J if you‚Äôre on a Windows machine or Cmd+J on MacOS"
  },
  {
    "objectID": "workshops/00_setup.html#option-2-terminal",
    "href": "workshops/00_setup.html#option-2-terminal",
    "title": "Essential Workshop Preparation",
    "section": "Option 2: Terminal",
    "text": "Option 2: Terminal\nNo additional setup required. SSH to cluster as usual, and we assume you are already familiar with command line if you decided to go with this option. üòÑ\n\nThis setup instruction is adapted from Customising Nf-Core Workshop materials from Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/2.1_customise_and_run.html",
    "href": "workshops/2.1_customise_and_run.html",
    "title": "Customising & running nf-core pipelines",
    "section": "",
    "text": "2.1.1. Pipeline setup\nIn this session we are using Singularity containers to manage software installation for all nf-core/rnaseq tools. Confirm the Singularity cache directory we set in the previous session using the $NXF_SINGULARITY_CACHEDIR Nextflow environmental variable:\necho $NXF_SINGULARITY_CACHEDIR\nüëÄ This should match the directory you set in the previous session:\n/config/binaries/singularity/containers_devel/nextflow\n\n\n2.1.2. Design your run command\nAs we learnt in lesson 1.2.4, all nf-core pipelines have a unique set of pipeline-specific parameters that can be used in conjunction with Nextflow parameters to configure the workflow. Generally, nf-core pipelines can be customised at a few different levels:\n\n\n\n\n\n\n\nLevel of effect\nCustomisation feature\n\n\n\n\nThe workflow\nWhere diverging methods are available for a pipeline, you may choose a path to follow\n\n\nA process\nWhere more than one tool is available for a single step, you may choose which to use\n\n\nA tool\nApply specific thresholds or optional flags for a tool on top of the default run command\n\n\nCompute resources\nSpecify resource thresholds or software execution methods for the workflow or a process\n\n\n\nAll nf-core pipelines are provided with comprehensive documentation that explain what the default workflow structure entails and options for customising this based on your needs. It is important to remember that nf-core pipelines typically do not include all possible tool parameters. This makes it challenging to piece these different sources of information together to determine which parameters you should be using.\nThe following sections of the documentation can be used to understand what the pipeline is doing and inform your choices about aspects of pipeline-specific customisations:\n\n\n\nDocs\nDescription\nCustomisation level\n\n\n\n\nIntroduction\nWorkflow summary\n\nworkflow\nprocess\n\n\n\nUsage\nInputs and options\n\nworkflow\nprocess\n\n\n\nParameters\nAvailable flags\n\nworkflow\nprocess\ncompute resources\n\n\n\nOutput\nFiles from all processes processes\n\nworkflow\nprocess\ntool\n\n\n\n\n\n\nChallenge\nView the parameters for the nf-core/rnaseq workflow using the command line for the specific version 3.11.1\n:::\n\nPipeline structure\nüëÄ Looking at the nf-core/rnaseq pipeline structure provided in the introduction, we can see that the developers have:\n\nOrganised the workflow into 5 stages based on the type of work that is being done\nProvided a choice of multiple methods and specified defaults\nProvided a choice of tool for some steps\n\n\n\n\n\n\n\n\nThoughts? üí≠\n\n\n\n‚ùì Observing the diagram above, which statement is true regarding the choice of alignment and quantification methods provided by the nf-core/rnaseq pipeline?\na. The pipeline uses a fixed method for read alignment and quantification.\nb. Users can choose between several different methods for read alignment and quantification.\nc. The pipeline always performs read alignment and quantification using STAR or HISAT2.\nd. The choice of alignment and quantification method is determined automatically based on the input data.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct answer is b. The nf-core/rnaseq pipeline allows users to choose between pseudo-alignment and quantification or several different methods for genome-based read alignment and quantification.\n\na is incorrect because the pipeline is not limited to a single method.\n\nc is incorrect because while read alignment and quantification using STAR and Salmon are the default method, users can choose pseudo-alignment method.\nd is also incorrect, as the pipeline only accepts fastq files as input and the choice of alignment and quantification method must be specified by the user.\n\n\n\n\n\n\n\nDefault pipeline usage\nThe number and type of default and optional parameters an nf-core pipeline accepts is at the discretion of it‚Äôs developers. However, at a minimum, nf-core pipelines typically:\n\nRequire users to specify a sample sheet (--input) detailing sample data and relevant metadata\nAutogenerate or acquire missing reference files from iGenomes ( using the --genome) if not provided by the user.\n\nYou can see the recommended (typical) run command and all the parameters available for the nf-core/rnaseq pipeline by running:\nnextflow run nf-core/rnaseq -r 3.11.1 --help \nThe typical or recommended run command for this pipeline is provided at the top of the screen:\n\nIt outlines a requirement for a few basic things:\n\nAn input samplesheet\nA location to store outputs\nRelevant reference data\nA software management method\n\n\n\n\n\n\n\nReminder: hyphens matter in Nextflow!\n\n\n\nNextflow-specific parameters use one (-) hyphen, whereas pipeline-specific parameters use two (--). In the typical run command above -profile is a Nextflow parameter, while --input is an nf-core parameter.\n\n\n\nRequired input: --input\nMost of us will need to adjust the default run command for our experiments. Today we‚Äôll be adjusting the typical nf-core/rnaseq run command by:\n\nProviding our own reference files\nUsing the Singularity software management profile, instead of Docker\nCustomising the execution of some processes\nSpecifying the computing resource limitations of our instances (2 CPUs, 8 Gb RAM)\n\nOur input fastq files (fastq/), reference data (ref/), and full sample sheet (samplesheet.csv) are already available on the cluster. Take a look at the files:\nls -l /data/seqliner/test-data/rna-seq\ntotal 24\ndrwxrwsr-x 2 jyu bioinf-core 8192 Sep 14 10:13 ERCC_index\ndrwxrwsr-x 2 jyu bioinf-core 8192 Sep 14 10:12 fastq\ndrwxrwsr-x 2 jyu bioinf-core 8192 Sep 14 10:12 ref\nTo make life easier store the path to our test data in a variable.\nmaterials=/data/seqliner/test-data/rna-seq\nGiven we are only testing the pipeline in this session, we only need to work with a couple of samples. Copy the first two samples from the full prepared sample sheet to a local version of the file:\nhead -n 3 $materials/samplesheet.csv &gt; samplesheet.csv\ncat samplesheet.csv\nsample,fastq_1,fastq_2,strandedness\nHBR_Rep1_ERCC,fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz,fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz,forward\nHBR_Rep2_ERCC,fastq/HBR_Rep2_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz,fastq/HBR_Rep2_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz,forward\n\n\nRequired input: reference data\nMany nf-core pipelines have a minimum requirement for reference data inputs. The input reference data requirements for this pipeline are provided in the usage documentation. We can replace the --genome flag in the typical run command with our own files. To see what reference files we can specify using parameters, rerun the pipeline‚Äôs help command to view all the available parameters:\nnextflow run nf-core/rnaseq -r 3.11.1 --help\nFrom the Reference genome options parameters, we will provide our own files using:\n\n--fasta $materials/ref/chr22_with_ERCC92.fa\n\n--gtf $materials/ref/chr22_with_ERCC92.gtf\n\n\n\n\n\n\n\nBeware the hidden parameters!\n\n\n\nNotice the message at the bottom of the screen:\n!! Hiding 24 params, use --show_hidden_params to show them !!\nKeep in mind that both this help command and the nf-core parameters documentation hides less common parameters.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nRe-run the help command to output the parameters for the nf-core/rnaseq pipeline and including all hidden parameters for version 3.11.1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the following:\nnextflow run nf-core/rnaseq -r 3.11.1 --help --show_hidden_params\n\n\n\n\n\n\nOptional parameters\nNow that we have prepared our input and reference data, we will customise the typical run command by:\n\nUsing Nextflow‚Äôs -profile parameter to specify that we will be running the Singularity profile instead of the Docker profile\nAdding additional process-specific flags to skip duplicate read marking, save trimmed reads and save unaligned reads\nAdding additional max resource flags to specify the number of CPUs and amount of memory available to the pipeline.\n\nThe parameters we will use are:\n\n-profile singularity\n--skip_markduplicates true\n--save_trimmed true\n--save_unaligned true\n--max_memory '6.GB'\n--max_cpus 2\n\nYou can see how we‚Äôve customised the typical run command in the diagram below:\n\n\n\n2.1.3. Run the pipeline\nWe will also create a separate output directory for this section.\ncd /scratch/users/&lt;your-username&gt;/nfWorkshop; mkdir ./lesson2 && cd $_\nNow that we have prepared our data and chosen which parameters to apply, run the pipeline:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    --input samplesheet.csv \\\n    --outdir ./lesson2.1 \\\n    --fasta $materials/ref/chr22_with_ERCC92.fa \\\n    --gtf $materials/ref/chr22_with_ERCC92.gtf \\\n    -profile singularity \\\n    --skip_markduplicates true \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2\nüëÄ Take a look at the stdout printed to the screen. Your workflow configuration and parameter customisations are all documented here. You can use this to confirm if your parameters have been correctly passed to the run command:\n\nAs the workflow starts, you will also see a number of processes spawn out underneath this. Recall from the earlier session that processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.\nTo understand how this is coordinated, consider the STAR_ALIGN process that is being run.\n\nüëÄ You‚Äôll notice a few things:\n\nWe can see which inputs are being processed by looking at the end of the process name\nWhen a process starts it progressively spawns tasks for all inputs to be processed\nA single TRIMGALORE process is run across both samples in our samplesheet.csv before STAR_ALIGN begins\nOnce a TRIMGALORE task is completed for a sample, the STAR_ALIGN task for that sample begins\nWhen the STAR_ALIGN process starts, it spawns 2 tasks.\n\n\n\n\n\n\n\nChallenge\n\n\n\nRecall from earlier Convert the parameter definitions into a YAML file, workshop-params.yaml, compatible with -params-file\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngtf: \"/data/seqliner/test-data/rna-seq/ref/chr22_with_ERCC92.gtf\"\nfasta: \"/data/seqliner/test-data/rna-seq/ref/chr22_with_ERCC92.fa\"\nskip_markduplicates: true\nsave_trimmed: true\nsave_unaligned: true\nmax_memory: \"6.GB\" \nmax_cpus: 2\ninput: \"samplesheet.csv\"\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core pipelines are provided with sensible default settings and required inputs.\nAn nf-core pipeline‚Äôs Usage, Output, and Parameters documentation can be used to design a suitable run command.\nParameters can be used to customise the workflow, processes, tools, and compute resources.\n\n\n\n\nIn the previous exercises, we have explored how to customise a run with workflow parameters on the command line or within a parameters file. In this lesson we will now look at configuration settings, which manage how the workflow is implemented on your system.\n\n\n\n\n\n\nNote\n\n\n\nNextflow‚Äôs portability is achieved by separating workflow implementation (input data, custom parameters, etc.) from the configuration settings (tool access, compute resources, etc.) required to execute it. This portability facilitates reproducibility: by applying the same parameters as a colleague, and adjusting configurations to suit your platform, you can achieve the same results on any machine with no requirement to edit the code.\n\n\n\n\n2.1.4. Default nf-core configuration\nTogether nextflow.config and base.config, define the default execution settings and parameters of an nf-core workflow.\nLet‚Äôs take a look at these two configuration files to gain an understanding of how defaults are applied. conf/base.config\nThe generic base.config sets the default compute resource settings to be used by the processes in the nf-core workflow. It uses process labels to enable different sets of resources to be applied to groups of processes that require similar compute. These labels are specified within the main.nf file for a process.\n We can over-ride these default compute resources using a custom configuration file.\nThen take a few moments to look through workflow/nextflow.config\nThe nextflow.config file is more workflow-specific, and sets the defaults for the workflow parameters, as well as defines profiles to change the default software access from $PATH to the specified access method, eg Singularity.\n We can over-ride these parameters on the command line or with a parameters file, and over-ride the default behaviour of searching for tools on $PATH by specifying a -profile.\nDefault settings for --max_cpus, --max_memory and --max_time are applied within the nf-core workflow/nextflow.config. These are generous values expecting to be over-ridden with your custom settings, to ensure that no single process attempts to use more resources than you have available on your platform.\nWithin workflow/conf/base.config, the check_max() function over-rides the process resources if the custom ‚Äòmax‚Äô setting is lower than the default setting for that process.\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nWhat are the default settings for CPU, memory and walltime for the STAR_ALIGN module?\nHow have these defaults been changed from our applied customisations in the previous runs?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo uncover these answers, we need to understand what process label has been assigned to the STAR_ALIGN module.\nSTAR_ALIGN has the label process_high which has the settings 12 CPUs, 72 GB mem, 16 hrs walltime applied by the default base.config. We have previosuly applied --max_cpus 2 and --max_memory 6.GB, so the check_max() function would have reduced the resources given to the STAR alignment process to 2 CPUs and 6 GB RAM, while retaining the default max walltime.\n\n\n\n\n\n2.1.5. When to use a custom config file\nIn our runs so far, we have avoided the need for a custom resource configuration file by:\n\nOver-riding the default tool access method of $PATH by specifying the singularity profile defined in workflow/nextflow.config\n\nWithout this, our runs for this workshop would fail because we do not have the workflow tools (eg STAR, salmon) installed locally on our VMs\n\nOver-riding the default values for CPUs and memory set in nextflow.config with --max_cpus 2 and --max_memory 6.GB to fit within our interactive sessions\n\nWithout these parameters, our runs would fail at the first process that requests more than this, because Nextflow workflows check that the requested resources are available before attempting to execute a workflow\n\n\nThese are basic configurations. What if:\n\nWe wanted to increase the resources used above what is requested with process labels to take advantage of high CPU or high memory infrastructures?\nWe wanted to run on a HPC or cloud infrastructure?\nWe wanted to execute specific modules on specific node types on a cluster?\nWe wanted to use a non-default software container?\nWe wanted to customise outputs beyond what was possible with the nf-core workflow parameters?\n\n\n\n2.1.6. Custom resource configuration using process labels\nCapping workflow resources using the max parameters is a bit of a blunt instrument.\nTo achieve optimum computational efficiency on your platform, more granular control may be required.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you appled --max_cpus 16 to the nf-core rnaseq workflow, the STAR_ALIGN module would still only utilise 12 CPUs, as this module (as we learnt in 2.1.5) has the label process_high which sets CPUs to 12.\nIf there were no processes with fulfilled input channels that could make use of the 4 remaining CPUs, those resources would sit idle until the STAR_ALIGN process had completed.\nOptimisation for this platform might for example set max_cpus to 8 so two samples could be aligned concurrently, or over-ride the CPU resources assigned to the STAR_ALIGN module to 16.\n\n\n\nThe next two lessons will demonstrate how to achieve this using custom configuration files that fine-tune resources using process labels to assign the same resources to groups of processes sharing the same label, or withName to target specific processes.\nIn order to do this, we need to use the process scope. Nextflow has a number of different scopes that can be included in configuration files, for example the params scope you covered in lesson 1.2.5 and applied to your config in lesson 2.1.8.\nWithin the process scope, we can configure resources and additional arguments for processes.\n\n\n\n\n\n\nWarning\n\n\n\nThe following exercise is trivial given the limitations of our interactive session. Consider how this approach can be really powerful when working on HPC or cloud infrastructures, where the executor and queue directives enable you to take full advantage of the compute resources available on your platform.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nüí° View the file conf/base.config for syntax example\nAdd a process scope inside the my_resources.config\nUse withLabel: &lt;label_name&gt; to set resources for each of the following labels:\n\nprocess_low\nprocess_medium\nprocess_high\n\n\n\n\nprocess {\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 2\n        memory = 6.GB\n    } \n    withLabel: process_high {\n        cpus = 2\n        memory = 6.GB\n    }\n}\nSave the file then re-run the workflow with our custom configuration, setting outdir parameter to lesson2.1.7:\n\n\n2.1.7. Examine the outputs\nOnce your pipeline has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 21-Nov-2023 03:31:24\nDuration    : 10m 50s\nCPU hours   : 0.3\nSucceeded   : 67\nThe pipeline ran successfully, however, note the warning about all samples having failed the strandedness check. We‚Äôll explore that in the next section.\nIn the meantime, list (ls -la) the contents of your directory, you‚Äôll see a few new directories (and a hidden directory and log file) have been created:\ntotal 356\ndrwxrwxr-x  6 rlupat rlupat   4096 Nov 22 03:33 .\ndrwxrwxr-x  7 rlupat rlupat   4096 Nov 22 03:14 ..\ndrwxrwxr-x  7 rlupat rlupat   4096 Nov 22 03:31 lesson2.1\ndrwxrwxr-x  4 rlupat rlupat   4096 Nov 22 03:31 .nextflow\n-rw-rw-r--  1 rlupat rlupat 283889 Nov 22 03:31 .nextflow.log\n-rw-rw-r--  1 rlupat rlupat  66150 Nov 22 03:20 .nextflow.log.1\n-rw-rw-r--  1 rlupat rlupat    492 Nov 22 03:15 samplesheet.csv\ndrwxrwxr-x 69 rlupat rlupat   4096 Nov 22 03:29 work\nüëÄ Nextflow has created 2 new output directories, work and lesson2.1 in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe lesson2.1 directory\nAll final outputs will be presented in a directory specified by the --outdir flag.\n\n\n\n\n\n\nBefore executing this run command\n\n\n\nIf you haven‚Äôt done so already, check that the run from lession 2.1.3 has completed successfully.\nYou run should have a summary message similar to below:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 11-May-2023 13:21:50\nDuration    : 7m 8s\nCPU hours   : 0.2\nSucceeded   : 66\nwith the following output directories:\n$ ls lesson2.1\nfastqc  multiqc  pipeline_info  star_salmon  trimgalore\n\n\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n  -profile singularity\n  -c my_resources.config \\\n  -params-file workshop-params.yaml \\\n  --outdir lesson2.1.6 \\\n  -resume\nüëÄ Notice that the Max job request options are no longer listed on the run log printed to screen, because we are setting them within the process scope rather than params scope.\n\n\n\n\n\n\nConfiguration order of priority\n\n\n\nThe order of priority in which parameters and configurations are applied by Nextflow.\nThe settings we specify with -c my_resources.config will over-ride those that appear in the default nf-core configurations workflow/nextflow.config and workflow/conf/base.config.\nSettings that are not over-ridden by -c &lt;config&gt; or any parameter from params file or provided on the command line will still be set to the nf-core defaults specified in nextflow.config, base.config or main.nf.\nTo avoid confusion, it is best not to name your custom configuration files nextflow.config!\n\n\n\n\n\n2.1.8. Custom resource configuration using process names\nThis exercise will demonstrate how to adjust the resource configurations for a specific process using the withName process selector, using the STAR_ALIGN module as example.\nwithName is a powerful tool:\n\nSpecifically target individual modules\nMultiple module names can be supplied using wildcards or ‚Äòor‚Äô (* or |) notation\nNo need to edit the module main.nf file to add a process label\nHas a higher priority than withLabel\n\nTo utilise withName, we first need to ensure we have the correct and specific executuion path for the module/modules that we wish to target.\nIdentify the execution path for the STAR_ALIGN module:\n\n\n\n\n\n\nFinding the module execution path\n\n\n\n\nThe extended execution path is built from the pipeline, workflow, subworkflow, and module names\nIt can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the view the modules.conf file on Github\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNote that this does not provide the PIPELINE or WORKFLOW name at the front of the path. You can add these manually (eg PIPELINE is NFCORE_RNASEQ and WORKFLOW is RNASEQ) but the path within modules.config is usually all that is required for specificity within a workflow\nIf you have previously run the pipeline, you could also extract the complete module execution path from your run log printed to screen, or the execution trace, timeline or report files within &lt;outdir&gt;/pipeline_info\n\n\n\nFor STAR_ALIGN within the nf-core/rnaseq workflow, any of the following would be correct and specific:\n'NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:STAR_ALIGN'\n'.*:RNASEQ:ALIGN_STAR:STAR_ALIGN'\n'.*:ALIGN_STAR:STAR_ALIGN'\n\nContinue editing my_resources.config. Inside the process scope, provide the execution path for the STAR_ALIGN module to the withName selector:\nprocess {\n  withName: '.*:RNASEQ:ALIGN_STAR:STAR_ALIGN' {\n  }\n}      \n\nThen set CPU to 24 and memory to 96 GB:\nprocess {\n  withName: '.*:RNASEQ:ALIGN_STAR:STAR_ALIGN' {\n    cpus = 24\n    memory = 96.GB\n  }\n} \n\n\n\n\n\n\nCompleted config file\n\n\n\n\n\nprocess {\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 2\n        memory = 6.GB\n    } \n    withLabel: process_high {\n        cpus = 2\n        memory = 6.GB\n    }\n    withName: '.*:RNASEQ:ALIGN_STAR:STAR_ALIGN' {\n        cpus = 24\n        memory = 96.GB\n    }\n}\n\n\n\n\n\n\n\n\n\nWhat if the parameter I want to apply isn‚Äôt available?\n\n\n\n\n\nRecall from earlier that nf-core modules use ext.args to pass additional arguments to a module. This uses a special Nextflow directive ext. If an nf-core pipeline does not have a pre-defined parameter for a process, you may be able to implement ext.args.\nThe inclusion of ext.args is currently best practice for all DSL2 nf-core modules where additional parameters may be required to run a process. However, this may not be implemented for all modules in all nf-core pipelines. Depending on the pipeline, these process modules may not have defined the ext.args variable in the script blocks and is thus not available for applying customisation. If that is the case consider submitting a feature request or a making pull request on the pipeline‚Äôs GitHub repository to implement this!\n\n\n\nSave the config then resume your run, setting outdir to lesson2.1.8, applying your custom resources from my_resources.config:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n  -profile singularity \\\n  -c my_resources.config \\\n  -params-file workshop-params.yaml \\\n  --outdir lesson2.1.8 \\\n  -resume \nIf your execution path for the STAR_ALIGN module was specified correctly, your run should have died with the error shown below because Nextflow checks that the resources requested are available before executing a workflow:\n\n\n\n\n\n\n\nThoughts? üí≠\n\n\n\nWhat do you expect would happen to your run if your execution path for the STAR_ALIGN module was not specified correctly?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn this case, our pipeline would complete OK, because the resources for the STAR_ALIGN process have been appropriately set for our interactive session using the process_high label within our my_resources.config.\nThe directives set within the withName scope would not be applicable, and a warning would be printed, eg\nWARN: There's no process matching config selector: .*:RNASEQ:ALIGN_STAR:STARALIGN\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nChange the container used by multiqc to quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0 using the withName scope in your my_resources.config\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nmy_resources.config\nprocess {\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 2\n        memory = 6.GB\n    } \n    withLabel: process_high {\n        cpus = 2\n        memory = 6.GB\n    }\n    withName: '.*:MULTIQC' {\n        container = \"quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0\"\n    }\n}\nRun the pipeline\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n  -profile singularity \\\n  -c my_resources.config \\\n  -params-file workshop-params.yaml \\\n  --outdir lesson2.1.8 \\\n  -resume \n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows work ‚Äòout of the box‚Äô but there are compute and software configurations we should customise for our runs to work well on our compute infrastructure\nnf-core executes by default with workflow/nextflow.config and workflow/conf/base.config and has a repository of community-contributed institutional config that ship with the workflow\ncustom config can be applied to a run with -c &lt;config_name&gt;, and will over-ride settings in the default config\ncustomisations can be targeted to specific processes using withLabel or withName\nworkflow parameters belong in -params-file &lt;params_file&gt; and not -c &lt;custom_config&gt;\n\n\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  }
]