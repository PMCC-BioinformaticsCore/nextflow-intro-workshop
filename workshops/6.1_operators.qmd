---
title: "**Nextflow Development - Channel Operators**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

::: callout-tip

### Objectives{.unlisted}

:::


## **Environment Setup**

Set up an interactive shell to run our Nextflow workflow: 

``` default
srun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash
```

Load the required modules to run Nextflow:

``` default
module load nextflow/23.04.1
module load singularity/3.7.3
```

Set the singularity cache environment variable:

```default
export NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow
```

Singularity images downloaded by workflow executions will now be stored in this directory.

You may want to include these, or other environmental variables, in your `.bashrc` file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found [here](https://www.nextflow.io/docs/latest/config.html#environment-variables).


## **6.1.1 Create Input Channels **

Previously, we created three Nextflow files:

```default
├── rnaseq.nf
├── modules.nf
└── modules
    └── trimgalore.nf
```

- `rnaseq.nf`: main workflow script where parameters are defined and processes were called. 

```default
#!/usr/bin/env nextflow

params.reads = "/scratch/users/.../training/nf-training/data/ggal/*_{1,2}.fq"
params.transcriptome_file = "/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa"

reads_ch = Channel.fromFilePairs("$params.reads")
reads_ch.view()

include { INDEX } from './modules.nf'
include { QUANTIFICATION as QT } from './modules.nf'
include { FASTQC as FASTQC_one } from './modules.nf'
include { FASTQC as FASTQC_two } from './modules.nf'
include { MULTIQC } from './modules.nf'
include { TRIMGALORE } from './modules/trimgalore.nf'

workflow {
  index_ch = INDEX(params.transcriptome_file)
  quant_ch = QT(index_ch, reads_ch)
  fastqc_ch = FASTQC_one(reads_ch)
  trimgalore_out_ch = TRIMGALORE(reads_ch).reads
  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)
  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)
}
```
- `modules.nf`: script containing the majority of modules, including `INDEX`, `QUANTIFICATION`, `FASTQC`, and `MULTIQC`

```default
process INDEX {
    container "/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img"

    input:
    path transcriptome

    output:
    path "salmon_idx"

    script:
    """
    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx
    """
}

process QUANTIFICATION {
    container "/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img"

    input:
    path salmon_index
    tuple val(sample_id), path(reads)

    output:
    path "$sample_id"

    script:
    """
    salmon quant --threads $task.cpus --libType=U \
    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id
    """
}

process FASTQC {
    container "/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img"

    input:
    tuple val(sample_id), path(reads)

    output:
    path "fastqc_${sample_id}_logs"

    script:
    """
    mkdir fastqc_${sample_id}_logs
    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}
    """
}

process MULTIQC {
    publishDir params.outdir, mode:'copy'
    container "/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img"

    input:
    path quantification
    path fastqc

    output:
    path "*.html"

    script:
    """
    multiqc . --filename $quantification
    """
}
```
- `modules/trimgalore.nf`: script inside a `modules` folder, containing only the `TRIMGALORE` process
```default
process TRIMGALORE {
  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' 

  input:
    tuple val(sample_id), path(reads)
  
  output:
    tuple val(sample_id), path("*{3prime,5prime,trimmed,val}*.fq.gz"), emit: reads
    tuple val(sample_id), path("*report.txt")                        , emit: log     , optional: true
    tuple val(sample_id), path("*unpaired*.fq.gz")                   , emit: unpaired, optional: true
    tuple val(sample_id), path("*.html")                             , emit: html    , optional: true
    tuple val(sample_id), path("*.zip")                              , emit: zip     , optional: true

  script:
    """
    trim_galore \\
      --paired \\
      --gzip \\
      ${reads[0]} \\
      ${reads[1]}
    """
}
```

In the workflow script `rnaseq.nf` we defined the `reads` parameter to be multiple paired `.fq` files, and the `transcriptome_file` parameter to be a single transcriptome file. A channel was created from the paired `.fq` files using the `fromFilePairs` channel factory, which created a tuple where the first element is a unique grouping key created based on similarities in file name, and the second element contains the list of paired files.

```default
#!/usr/bin/env nextflow

params.reads = "/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq"
params.transcriptome_file = "/scratch/users/.../nf-training/ggal/transcriptome.fa"

reads_ch = Channel.fromFilePairs("$params.reads")
reads_ch.view()

```
Run the pipeline:

```default
>>> nextflow run rnaseq.nf --outdir output
...
[gut, [/home/sli/nextflow_training/training/nf-training/data/ggal/gut_1.fq, /home/sli/nextflow_training/training/nf-training/data/ggal/gut_2.fq]]
[liver, [/home/sli/nextflow_training/training/nf-training/data/ggal/liver_1.fq, /home/sli/nextflow_training/training/nf-training/data/ggal/liver_2.fq]]
[lung, [/home/sli/nextflow_training/training/nf-training/data/ggal/lung_1.fq, /home/sli/nextflow_training/training/nf-training/data/ggal/lung_2.fq]]
```

## **6.1.2 `map` **

The map operator applies a mapping function to each item in a channel. This function is expressed using the Groovy closure `{ }`. 

For example:
```default
Channel
    .of('hello', 'world')
    .map { word -> [word, word.size()] }
    .view()
```
In this example, a channel containing the strings `hello` and `world` is created. 

Inside the `map` operator, `word` is used as the variable to represent the value that is passed to the operator, ie. each element in the channel, `hello` and `world`.

The function defined by the `map` operator returns a tuple, where the first element is the string represented by the `word` variable, and the second element is the length of the string, emitted by the `size()` Groovy method. 


Output:
```default
[hello, 5]
[world, 5]
```

For our RNA-seq pipeline, let's first create separate transcriptome files for each organ: `lung.transcriptome.fa`, `liver.transcriptome.fa`, `gut.transcriptome.fa`

```default
cp "/scratch/users/.../nf-training/data/ggal/transcriptome.fa" "/scratch/users/.../nf-training/data/ggal/lung.transcriptome.fa"
cp "/scratch/users/.../nf-training/data/ggal/transcriptome.fa" "/scratch/users/.../nf-training/data/ggal/liver.transcriptome.fa"
mv "/scratch/users/.../nf-training/data/ggal/transcriptome.fa" "/scratch/users/.../nf-training/data/ggal/gut.transcriptome.fa"
```

Ensure `transcriptome.fa` no longer exists: 

```default
>>> ls /scratch/users/.../nf-training/data/ggal/
gut_1.fq
gut_2.fq
gut.transcriptome.fa
liver_1.fq
liver_2.fq
liver.transcriptome.fa
lung_1.fq
lung_2.fq
lung.transcriptome.fa
```


**Exercise** 

In the `rnaseq.nf` script, set the `transcriptome_file` parameter to match for all three `.fa` files using a glob path matcher.

Use the `fromPath` channel factory to read in the transcriptome files, and the `map` operator to create a tuple where the first element is the organ of the `.fa`, and the second element is the path of the `.fa` file. Assign the final output to be a channel called `transcriptome_ch`.

The `getSimpleName()` Groovy method can be used extract the organ name from the `.fa` file:

```default
>>> println file("/scratch/users/.../nf-training/data/ggal/lung.transcriptome.fa").getSimpleName()
lung
```
Use the `view()` channel operator to view the `transcriptome_ch` channel. The expected output:
```default
[lung, /scratch/users/.../nf-training/data/ggal/lung.transcriptome.fa]
[liver, /scratch/users/.../nf-training/data/ggal/liver.transcriptome.fa]
[gut, /scratch/users/.../nf-training/data/ggal/gut.transcriptome.fa]
```

::: {.callout-note appearance="simple" collapse="true"}
### Solution

The `transcriptome_file` parameter is defined using `*`, using glob to match for all three `.fa` files. The `fromPath` channel factory is used to read the `.fa` files, and the `map` operator is used to create the tuple. 

In the `map` function, the variable `fasta` was chosen to represent each element that is passed to the function. The function emits a tuple where the first element is the organ name, returned by the `getSimpleName()` method, and the second element is the `.fa` file path. 

```default
params.transcriptome_file = "/scratch/users/.../nf-training/data/ggal/*.fa"

transcriptome_ch = Channel.fromPath("$params.transcriptome_file")
    .map { fasta -> [fasta.getSimpleName(), fasta]}
    .view()
```
:::

</br>

**Challenge** 

Modify the `INDEX` process to accomodate the input structure of `transcriptome_ch`. Modify the output of `INDEX` so that a tuple is emitted, where the first elememt is the value of the grouping key, and the second element is the path of the `salmon_idx` folder.


::: {.callout-note appearance="simple" collapse="true"}
### Solution

The input is now defined to be a tuple of two elements, where the first element is the grouping key and the second element is the path of the transcriptome file. 

```default
process INDEX {
    container "/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img"

    input:
    tuple val(sample_id), path(transcriptome)

    output:
    tuple val(sample_id), path("salmon_idx")

    script:
    """
    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx
    """
}
```
:::

</br>

## **6.1.3 `join` **

The `join` operator emits the inner product of two channels of tuples, using a matching grouping key.

For example:
```default
left  = Channel.of( ['X', 1], ['Y', 2], ['Z', 3], ['P', 7] )
right = Channel.of( ['Z', 6], ['Y', 5], ['X', 4] )

left.join(right).view()
[Z, 3, 6]
[Y, 2, 5]
[X, 1, 4]
```

**Exercise** 

In the RNA-seq pipeline, index `transcriptome_ch` using the `INDEX` process. Emit the output as `index_ch`.

Create a channel `quant_inputs_ch` that contains the `reads_ch` joined onto the `index_ch` using a matching key. The expected output:
```default
[liver, /scratch/users/.../work/cf/42458b80e050a466d62baf99d0c1cf/salmon_idx, [/scratch/users/.../nf-training/data/ggal/liver_1.fq, /scratch/users/.../nf-training/data/ggal/liver_2.fq]]
[lung, /scratch/users/.../work/64/90a77a5f1ed5a0000f6620fd1fab9a/salmon_idx, [/scratch/users/.../nf-training/data/ggal/lung_1.fq, /scratch/users/.../nf-training/data/ggal/lung_2.fq]]
[gut, /scratch/users/.../work/37/352b00bfb71156a9250150428ddf1d/salmon_idx, [/scratch/users/.../nf-training/data/ggal/gut_1.fq, /scratch/users/.../nf-training/data/ggal/gut_2.fq]]
```

Use `quant_inputs_ch` as the input for the `QT` process. Modify the process such that the input will be a tuple consisting of three elements, where the first element is the grouping key, the second element is the salmon index and the third element is the list of `.fq` reads. Also modify the output of the `QT` process to emit a tuple of two elements, where the first element is the grouping key and the second element is the `$sample_id` folder. Emit the output as `quant_ch` in the `workflow` block.

::: {.callout-note appearance="simple" collapse="true"}
### Solution

The `reads_ch` is joined onto the `index_ch` using the `join` channel operator, and is assigned to the channel `quant_inputs_ch`. The new `quant_inputs_ch` channel is input into the `QT` process. 

```default
workflow {
  index_ch = INDEX(transcriptome_ch)

  quant_inputs_ch = index_ch.join(reads_ch)
  quant_ch = QT(quant_inputs_ch)
}
```

The `input` has been modified to be a tuple of three elements - the first element is the grouping key, the second element is the path to the salmon index, and the third element is the list of `.fq` reads.

```default
process QUANTIFICATION {
    container "/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img"

    input:
    tuple val(sample_id), path(salmon_index), path(reads)

    output:
    tuple val(sample_id) path("$sample_id")

    script:
    """
    salmon quant --threads $task.cpus --libType=U \
    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id
    """
}
```

:::


KEEP??????????

## **6.1. `combine` **

The `combine` operator is similar to the `join` operator

produces the combinations (i.e. cross product, “Cartesian” product) of two source channels, or a channel and a list (as the right operand), emitting each combination separately.

For example:

numbers = Channel.of(1, 2, 3)
words = Channel.of('hello', 'ciao')

numbers
    .combine(words)
    .view()
[1, hello]
[2, hello]
[3, hello]
[1, ciao]
[2, ciao]
[3, ciao]
The by option can be used to combine items that share a matching key. The value should be the zero-based index of the tuple, or a list of indices. For example:

source = Channel.of( [1, 'alpha'], [2, 'beta'] )
target = Channel.of( [1, 'x'], [1, 'y'], [1, 'z'], [2, 'p'], [2, 'q'], [2, 't'] )

source.combine(target, by: 0).view()
[1, alpha, x]
[1, alpha, y]
[1, alpha, z]
[2, beta, p]
[2, beta, q]
[2, beta, t]
Note

The combine operator is similar to cross and join, making them easy to confuse. Their differences can be summarized as follows:

combine and cross both produce an outer product or cross product, whereas join produces an inner product.
combine filters pairs with a matching key only if the by option is used, whereas cross always filters pairs with a matching key.
combine with the by option merges and flattens each pair, whereas cross does not. Compare the examples for combine and cross to see this difference.


## **6.0. `groupTuple` **

Returns: queue channel

The groupTuple operator collects lists (i.e. tuples) from a source channel into groups based on a grouping key. A new tuple is emitted for each distinct key.

To be more precise, the operator transforms a sequence of tuples like (K, V, W, ..) into a sequence of tuples like (K, list(V), list(W), ..).

For example:

Channel.of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )
    .groupTuple()
    .view()
[1, [A, B, C]]
[2, [C, A]]
[3, [B, D]]
By default, the first element of each tuple is used as the grouping key. The by option can be used to specify a different index, or list of indices. For example, to group by the second element of each tuple:

Channel.of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )
    .groupTuple(by: 1)
    .view()
[[1, 2], A]
[[1, 3], B]
[[2, 1], C]
[[3], D]
By default, if you don’t specify a size, the groupTuple operator will not emit any groups until all inputs have been received. If possible, you should always try to specify the number of expected elements in each group using the size option, so that each group can be emitted as soon as it’s ready. In cases where the size of each group varies based on the grouping key, you can use the built-in groupKey() function, which allows you to define a different expected size for each group:

chr_frequency = ["chr1": 2, "chr2": 3]

Channel.of(
        ['region1', 'chr1', '/path/to/region1_chr1.vcf'],
        ['region2', 'chr1', '/path/to/region2_chr1.vcf'],
        ['region1', 'chr2', '/path/to/region1_chr2.vcf'],
        ['region2', 'chr2', '/path/to/region2_chr2.vcf'],
        ['region3', 'chr2', '/path/to/region3_chr2.vcf']
    )
    .map { region, chr, vcf -> tuple( groupKey(chr, chr_frequency[chr]), vcf ) }
    .groupTuple()
    .view()
[chr1, [/path/to/region1_chr1.vcf, /path/to/region2_chr1.vcf]]
[chr2, [/path/to/region1_chr2.vcf, /path/to/region2_chr2.vcf, /path/to/region3_chr2.vcf]]
Available options:

by
The zero-based index of the element to use as the grouping key. Can also be a list of indices, e.g. by: [0,2] (default: [0]).
remainder
When true, incomplete tuples (i.e. groups with less than size items) are emitted as partial groups, otherwise they are discarded (default: false). This option can only be used with size.
size
The required number of items for each group. When a group reaches the required size, it is emitted.
sort
Defines the sorting criteria for the grouped items. Can be one of the following values:

false: No sorting is applied (default).
true: Order the grouped items by the item’s natural ordering i.e. numerical for number, lexicographic for string, etc. See the Java documentation for more information.
'hash': Order the grouped items by the hash number associated to each entry.
'deep': Similar to the previous, but the hash number is created on actual entries content e.g. when the item is a file, the hash is created on the actual file content.
A custom sorting criteria used to order the nested list elements of each tuple. It can be a Closure or a Comparator object.


copy same fastq data into lane 2 

```default
#!/usr/bin/env nextflow

params.reads_lane2 = "/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq"
reads_lane2_ch = Channel.fromFilePairs("$params.reads_lane2")
reads_lane2_ch.view()

```

has same grouping key. also assign trancrptome and input into process (create module)


```default

include {QUANTIFICATION as QUANTIFICATION1} from ../vcadihvidsv
include {QUANTIFICATION as QUANTIFICATION2} from ../vcadihvidsv


process MERGE_QUANTIFICATION {
    input:
    tuple val(meta), path(quant1), path(quant2)

    output:
    tuple val(meta), path("merged")

    script:

    """
    mkdir merged
    cp quant1/* \$merged
    cp quant2/* \$merged

    """

}

```

workflow block: (still need to change downstream processes FASTQC + MULTIQC)

```default
workflow {
  index_ch = INDEX(transcriptome_ch)

  inputs_ch = reads_ch.join(index_ch)

  reads_lane2_ch = Channel.fromFilePairs("$params.reads_lane2")
  reads_lane2_ch.view()

  inputs2_ch = reads_lane2_ch.join(index_ch)

  quant_ch = QUANTIFICATION1(inputs_ch)
  quant_ch.view()

  quant2_ch = QUANTIFICATION2(inputs2_ch)
  quant2_ch.view()

  quant_joined = quant_ch.join(quant2_ch)
  merged_quant_ch = MERGE_QUANTIFICATION(quant_joined)

  fastqc_ch = FASTQC(reads_ch)
  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)
}

```




## **6.0. branch **


Returns: multiple queue channels or value channels, matching the source type

The branch operator forwards each item from a source channel to one of multiple output channels, based on a selection criteria.

The selection criteria is a closure that defines, for each output channel, a unique label followed by a boolean expression. When an item is received, it is routed to the first output channel whose expression evaluates to true. For example:

Channel.of(1, 2, 3, 40, 50)
    .branch {
        small: it < 10
        large: it > 10
    }
    .set { result }

result.small.view { "$it is small" }
result.large.view { "$it is large" }
1 is small
2 is small
3 is small
40 is large
50 is large 

The value emitted to each branch can be customized with an expression statement (or statements) after the branch condition:

Channel.of(1, 2, 3, 40, 50)
    .branch {
        foo: it < 10
            return it+2

        bar: it < 50
            return it-2

        other: true
            return 0
    }
    .set { result }

result.foo.view { "$it is foo" }
result.bar.view { "$it is bar" }
result.other.view { "$it is other" }
3 is foo
4 is foo
5 is foo
38 is bar

When the return keyword is omitted, the value of the last expression statement is implicitly returned.
The branchCriteria() method can be used to create a branch criteria as a variable that can be passed as an argument to any number of branch operations, as shown below:

def criteria = branchCriteria {
    small: it < 10
    large: it > 10
}

Channel.of(1, 2, 30).branch(criteria).set { ch1 }
Channel.of(10, 20, 3).branch(criteria).set { ch2 }

ch1.small.view { "$it is small" }
ch1.large.view { "$it is large" }
ch2.small.view { "$it is small" }
ch2.large.view { "$it is large" }
1 is small
2 is small
3 is small
20 is large
30 is large




read in all inouts together, then separate by file type

```default
#!/usr/bin/env nextflow

params.reads = "/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq"
reads_ch = Channel.fromFilePairs("$params.reads")
reads_ch.view

reads_ch.branch {meta, fastqs ->
    single: fastqs.size() = 1
        meta.paired = false
        return [meta, fastqs]
    multiple: fastqs.size() > 1
        meta.paired = true
        return [meta, fastqs]

}

reads_ch.single.view()
reads_ch.paired.view()

```

Use paired reads for one process, different process for nonpaired....




---
^*This workshop is adapted from [Fundamentals Training](https://training.nextflow.io/basic_training/), [Advanced Training](https://training.nextflow.io/advanced/), [Developer Tutorials](https://nf-co.re/docs/contributing/tutorials/creating_with_nf_core#creating-a-pipeline), [Nextflow Patterns](https://nextflow-io.github.io/patterns/) materials from Nextflow, nf-core [nf-core tools documentation](https://nf-co.re/docs/nf-core-tools) and [nf-validation](https://nextflow-io.github.io/nf-validation/)*^
